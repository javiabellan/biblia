<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>Introduction - Documentación de Javi</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Introduction";
        var mkdocs_page_input_path = "3_c/embedded systems/TinyML (tensorflow-lite-micro)/readme.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> Documentación de Javi
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/1_dirs.md">📁 Dirs (cd,ls,find,ranger)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/2_files.md">📄 Files (cat,file,stat)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/3_process.md">⏳ Process (ps,top)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/compression.md">🗜️ Compress (zip,tar,bz)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/cryptography.md">🔑 Cryptography (base64)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/plotting.md">📊 Plotting (gnuplot)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/scripting.md">👨‍💻 Scripting</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/customization.md">🎨 Customization (dotfiles)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/git.md">GIT</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/editors.md">📝 Editors (nano,vim,jupyter)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/pacman.md">📦 Software (pacman,aur,pip)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/ssh.md">SSH</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools/tmux.md">TMUX</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI data tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/free_text.md">⚪️ Free text (regex,grep,tr,sed)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/csv.md">🟢 Excel,csv,tsv (cut,paste,awk)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/html.md">🟡 HTML (pup) XML</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/json.md">🟠 JSON (jq)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/pdf.md">🔴 PDF (Xpdf,poppler)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/image.md">🔵 Image (ImageMagick)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_data/sound_video.md">🟣 Sound,Video (ffmepg,sox)</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI networking tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/web_scraping.md">⬇️ Web Scraping (curl,wget)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/pentesting.md">🗡️ Pentesting (nmap)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/deffensive.md">🛡️ Deffensive</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/dns.md">📒 DNS (dig,nslookup,whois)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/vpn.md">🔒 VPN</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../cli_tools_net/firewall.md">📛 Firewall</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Python</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../../python/numpy.md">🟦 Numpy</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/machine_learning.md">🟫 Sklearn</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/pytorch.md">🟥 Pytorch</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/tensorflow.md">🟧 Tensorflow</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/onnx.md">⬜️ ONNX</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/tensorrt.md">🟩 TensorRT</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../python/numpy.md">🟦 Parallel</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">C++</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../../../programming/ruby.md">♦️ Ruby</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../programming/monitoring.md">Monitoring</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../programming/profiling.md">Profiling</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../programming/debugging.md">Debugging</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../../../programming/makefile.md">Makefile</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">Documentación de Javi</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
      <li>Introduction</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="introduction">Introduction</h1>
<p>TensorFlow Lite for Microcontrollers is written in <strong>C++ 11</strong> and requires a <strong>32-bit platform</strong>. It has been tested extensively with many processors based on the Arm Cortex-M Series architecture, and has been ported to other architectures including ESP32.</p>
<h2 id="step-1-train-a-model">STEP 1: Train a model</h2>
<p>Train a small TensorFlow model that can fit your target device and contains supported operations.</p>
<blockquote>
<p>Note if you are going to apply post-training quantization later, you can also apply <strong>quantization aware training</strong> in this step.</p>
</blockquote>
<h2 id="step-2-convert-to-tensorflow-lite">STEP 2: <a href="https://www.tensorflow.org/lite/convert">Convert to TensorFlow Lite</a></h2>
<p>Convert to a TensorFlow Lite model using the TensorFlow Lite converter.</p>
<p>By default, the weights and biases in a model are stored as 32-bit floating-point numbers (there is no quantization)</p>
<pre><code class="language-python">converter    = tf.lite.TFLiteConverter.from_keras_model(my_keras_model)    # OPTION A
converter    = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)  # OPTION B
tflite_model = converter.convert()
open(&quot;my_model_float32.tflite&quot;, &quot;wb&quot;).write(tflite_model) # Save the model to disk
</code></pre>
<p>Convert the model with Full integer quantization. (4x smaller, 3x+ speedup)</p>
<pre><code class="language-python">def representative_dataset():
  for i in range(500):
    yield([x_train[i].reshape(1, 1)])

converter.optimizations             = [tf.lite.Optimize.DEFAULT] # Set the optimization flag.
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] 
converter.inference_input_type      = tf.int8
converter.inference_output_type     = tf.int8
converter.representative_dataset    = representative_dataset # Provide a representative dataset to ensure we quantize correctly.

tflite_model = converter.convert()
open(&quot;my_model_int8.tflite&quot;, &quot;wb&quot;).write(tflite_model) # Save the model to disk
</code></pre>
<h3 id="3-convert-to-tensorflow-micro">3. Convert to TensorFlow Micro</h3>
<p>Convert to a C byte array using standard tools to store it in a read-only program memory on device.</p>
<pre><code class="language-python">
MODEL_TFLITE = &quot;my_model_int8.tflite&quot;
MODEL_MICRO  = &quot;my_model_int8.cc&quot;

# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model
#!xxd -i my_model_int8.tflite &gt; my_model_int8.cc
!xxd -i {MODEL_TFLITE} &gt; {MODEL_MICRO}

# Update variable names
REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')
!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_MICRO}
</code></pre>
<h2 id="4-inference-on-c11">4. Inference on c++11</h2>
<p>Run inference on device using the C++ library and process the results.</p>
<pre><code class="language-c++">// 1. Include the library headers
#include &lt;math.h&gt;
#include &quot;tensorflow/lite/micro/all_ops_resolver.h&quot;     // provides the operations used by the interpreter to run the model.
#include &quot;tensorflow/lite/micro/micro_error_reporter.h&quot; // outputs debug information.
#include &quot;tensorflow/lite/micro/micro_interpreter.h&quot;    // contains code to load and run models.
#include &quot;tensorflow/lite/micro/testing/micro_test.h&quot;
#include &quot;tensorflow/lite/schema/schema_generated.h&quot;    // contains the schema for the TensorFlow Lite FlatBuffer model file format.

// 2. Include the model header
#include &quot;tensorflow/lite/micro/examples/hello_world/hello_world_model_data.h&quot;


////////////////////////////////////// 3. Include the unit test framework header
TF_LITE_MICRO_TESTS_BEGIN

TF_LITE_MICRO_TEST(LoadModelAndPerformInference) {
  // Define the input and the expected output
  float x = 0.0f;
  float y_true = sin(x);

  // Set up logging
  tflite::MicroErrorReporter micro_error_reporter;

  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  const tflite::Model* model = ::tflite::GetModel(g_hello_world_model_data);
  if (model-&gt;version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(&amp;micro_error_reporter,
                         &quot;Model provided is schema version %d not equal &quot;
                         &quot;to supported version %d.\n&quot;,
                         model-&gt;version(), TFLITE_SCHEMA_VERSION);
  }

  // This pulls in all the operation implementations we need
  tflite::AllOpsResolver resolver;

  constexpr int kTensorArenaSize = 2000;
  uint8_t tensor_arena[kTensorArenaSize];

  // Build an interpreter to run the model with
  tflite::MicroInterpreter interpreter(model, resolver, tensor_arena,
                                       kTensorArenaSize, &amp;micro_error_reporter);
  // Allocate memory from the tensor_arena for the model's tensors
  TF_LITE_MICRO_EXPECT_EQ(interpreter.AllocateTensors(), kTfLiteOk);

  // Obtain a pointer to the model's input tensor
  TfLiteTensor* input = interpreter.input(0);

  // Make sure the input has the properties we expect
  TF_LITE_MICRO_EXPECT_NE(nullptr, input);
  // The property &quot;dims&quot; tells us the tensor's shape. It has one element for
  // each dimension. Our input is a 2D tensor containing 1 element, so &quot;dims&quot;
  // should have size 2.
  TF_LITE_MICRO_EXPECT_EQ(2, input-&gt;dims-&gt;size);
  // The value of each element gives the length of the corresponding tensor.
  // We should expect two single element tensors (one is contained within the
  // other).
  TF_LITE_MICRO_EXPECT_EQ(1, input-&gt;dims-&gt;data[0]);
  TF_LITE_MICRO_EXPECT_EQ(1, input-&gt;dims-&gt;data[1]);
  // The input is an 8 bit integer value
  TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt8, input-&gt;type);

  // Get the input quantization parameters
  float input_scale = input-&gt;params.scale;
  int input_zero_point = input-&gt;params.zero_point;

  // Quantize the input from floating-point to integer
  int8_t x_quantized = x / input_scale + input_zero_point;
  // Place the quantized input in the model's input tensor
  input-&gt;data.int8[0] = x_quantized;

  // Run the model and check that it succeeds
  TfLiteStatus invoke_status = interpreter.Invoke();
  TF_LITE_MICRO_EXPECT_EQ(kTfLiteOk, invoke_status);

  // Obtain a pointer to the output tensor and make sure it has the
  // properties we expect. It should be the same as the input tensor.
  TfLiteTensor* output = interpreter.output(0);
  TF_LITE_MICRO_EXPECT_EQ(2, output-&gt;dims-&gt;size);
  TF_LITE_MICRO_EXPECT_EQ(1, output-&gt;dims-&gt;data[0]);
  TF_LITE_MICRO_EXPECT_EQ(1, output-&gt;dims-&gt;data[1]);
  TF_LITE_MICRO_EXPECT_EQ(kTfLiteInt8, output-&gt;type);

  // Get the output quantization parameters
  float output_scale = output-&gt;params.scale;
  int output_zero_point = output-&gt;params.zero_point;

  // Obtain the quantized output from model's output tensor
  int8_t y_pred_quantized = output-&gt;data.int8[0];
  // Dequantize the output from integer to floating-point
  float y_pred = (y_pred_quantized - output_zero_point) * output_scale;

  // Check if the output is within a small range of the expected output
  float epsilon = 0.05f;
  TF_LITE_MICRO_EXPECT_NEAR(y_true, y_pred, epsilon);

  // Run inference on several more values and confirm the expected outputs
  x = 1.f;
  y_true = sin(x);
  input-&gt;data.int8[0] = x / input_scale + input_zero_point;
  interpreter.Invoke();
  y_pred = (output-&gt;data.int8[0] - output_zero_point) * output_scale;
  TF_LITE_MICRO_EXPECT_NEAR(y_true, y_pred, epsilon);

  x = 3.f;
  y_true = sin(x);
  input-&gt;data.int8[0] = x / input_scale + input_zero_point;
  interpreter.Invoke();
  y_pred = (output-&gt;data.int8[0] - output_zero_point) * output_scale;
  TF_LITE_MICRO_EXPECT_NEAR(y_true, y_pred, epsilon);

  x = 5.f;
  y_true = sin(x);
  input-&gt;data.int8[0] = x / input_scale + input_zero_point;
  interpreter.Invoke();
  y_pred = (output-&gt;data.int8[0] - output_zero_point) * output_scale;
  TF_LITE_MICRO_EXPECT_NEAR(y_true, y_pred, epsilon);
}

TF_LITE_MICRO_TESTS_END
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li>https://www.tensorflow.org/lite/microcontrollers/get_started_low_level</li>
<li>https://github.com/Benja1972/TinyML_arduino</li>
</ul>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script>var base_url = '../../../..';</script>
    <script src="../../../../js/theme_extra.js" defer></script>
    <script src="../../../../js/theme.js" defer></script>
      <script src="../../../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
