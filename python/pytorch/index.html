<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>ğŸŸ¥ Pytorch - DocumentaciÃ³n de Javi</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\ud83d\udfe5 Pytorch";
        var mkdocs_page_input_path = "python/pytorch.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> DocumentaciÃ³n de Javi
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/1_dirs/">ğŸ“ Dirs (cd,ls,find,ranger)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/2_files/">ğŸ“„ Files (cat,file,stat)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/3_process/">â³ Process (ps,top)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/compression/">ğŸ—œï¸ Compress (zip,tar,bz)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/cryptography/">ğŸ”‘ Cryptography (base64)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/plotting/">ğŸ“Š Plotting (gnuplot)</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../cli_tools/scripting.md">ğŸ‘¨â€ğŸ’» Scripting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/customization/">ğŸ¨ Customization (dotfiles)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/git/">GIT</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../cli_tools/editors.md">ğŸ“ Editors (nano,vim,jupyter)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/pacman/">ğŸ“¦ Software (pacman,aur,pip)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/ssh/">SSH</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools/tmux/">TMUX</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI data tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/free_text/">âšªï¸ Free text (regex,grep,tr,sed)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/csv/">ğŸŸ¢ Excel,csv,tsv (cut,paste,awk)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/html/">ğŸŸ¡ HTML (pup) XML</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/json/">ğŸŸ  JSON (jq)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/pdf/">ğŸ”´ PDF (Xpdf,poppler)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/image/">ğŸ”µ Image (ImageMagick)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_data/sound_video/">ğŸŸ£ Sound,Video (ffmepg,sox)</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CLI networking tools</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_net/web_scraping/">â¬‡ï¸ Web Scraping (curl,wget)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_net/pentesting/">ğŸ—¡ï¸ Pentesting (nmap)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_net/deffensive/">ğŸ›¡ï¸ Deffensive</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../cli_tools_net/dns.md">ğŸ“’ DNS (dig,nslookup,whois)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_net/vpn/">ğŸ”’ VPN</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cli_tools_net/firewall/">ğŸ“› Firewall</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Python</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../numpy/">ğŸŸ¦ Numpy</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../machine_learning.md">ğŸŸ« Sklearn</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">ğŸŸ¥ Pytorch</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="../tensorflow.md">ğŸŸ§ Tensorflow</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../onnx.md">â¬œï¸ ONNX</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../tensorrt.md">ğŸŸ© TensorRT</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../numpy/">ğŸŸ¦ Parallel</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">C++</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../programming/ruby/">â™¦ï¸ Ruby</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../programming/monitoring/">Monitoring</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../programming/profiling/">Profiling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../programming/debugging/">Debugging</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../programming/makefile/">Makefile</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">DocumentaciÃ³n de Javi</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Python &raquo;</li>
      <li>ğŸŸ¥ Pytorch</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="naive-implementatation">Naive implementatation</h2>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read image 1â”‚ â”‚Read image 2â”‚ â”‚Read image 3â”‚                                 â”‚Read image 4â”‚ ...
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU                                            â”‚Copy to GPUâ”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 GPU                                                             â”‚INFER MODEL         â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="read-images-faster">Read images faster</h2>
<h3 id="hardaware">Hardaware</h3>
<p>Try to store your dataset in a fast storage</p>
<ol>
<li>RAM</li>
<li>SSDs NVMe RAID</li>
<li>SSD NVMe</li>
<li>SSDs SATA RAID</li>
<li>SSD SATA</li>
<li>HDD RAID</li>
<li>HDD SATA</li>
<li>Other machine in local network (/mnt/media mounting point)</li>
<li>Internet</li>
</ol>
<h3 id="option-fast-fast-imgedecodin-libjpg-turbo">Option fast: Fast imgedecodin <code>libjpg-turbo</code></h3>
<ul>
<li>Using <a href="https://github.com/ajkxyz/jpeg4py">jpeg4py</a>: <code>jpeg.JPEG(img).decode()</code> instead of <code>np.array(Image.open(img))</code></li>
<li><a href="https://www.pankesh.com/posts/2019-05-02-pytorch-augmentation-with-libjpeg-turbo">example of usage</a></li>
<li>Using <code>Pillow&gt;=9.0.0</code> <a href="https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html#switched-to-libjpeg-turbo-in-macos-and-linux-wheels">source</a></li>
</ul>
<p>Notese que a partir de la la verion 9 de PIL, viene por defecto </p>
<blockquote>
<p>Comprobar que PIL usa libjpg-turbo
<code>python
import PIL.features
print(PIL.features.check_feature("libjpeg_turbo"))</code></p>
</blockquote>
<h3 id="even-faster">Even faster:</h3>
<p><strong>Precompute tensors and save the on disk</strong></p>
<p><strong>For reading just memap, No need to decoding</strong></p>
<ul>
<li>Numpy Memmap</li>
<li>Torch storage</li>
<li><a href="https://ffcv.io">FFCV</a></li>
</ul>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read img1â”‚ â”‚Read img2â”‚ â”‚Read img3â”‚                                      â”‚Read img4â”‚ ...
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU                                   â”‚Pinâ”‚â”‚Copy to GPUâ”‚
                                            â””â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 GPU                                                          â”‚INFER MODEL         â”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="optimization-avoid-unnecessary-host-copies-pin_memorytrue">Optimization: Avoid unnecessary host copies <code>pin_memory=True</code></h2>
<p>Host (CPU) data allocations are pageable by default. The GPU cannot access data directly from pageable host memory, so when a data transfer from pageable host memory to device memory is invoked, the CUDA driver must first allocate a temporary page-locked, or â€œpinnedâ€, host array, copy the host data to the pinned array, and then transfer the data from the pinned array to device memory, as illustrated below.</p>
<ul>
<li>https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/</li>
</ul>
<p>if <code>pin_memory=True</code> is set on the Pytorch's dataloader, it will copy Tensors into device/CUDA pinned memory before returning them.</p>
<p>Also, once you pin a tensor or storage, you can use asynchronous GPU copies. Just pass an additional <code>non_blocking=True</code> argument to a <code>to()</code> or a <code>cuda()</code> call. This can be used to overlap data transfers with computation. (Prefetching on GPU Optimization).</p>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read img1â”‚ â”‚Read img2â”‚ â”‚Read img3â”‚                                â”‚Read img4â”‚ ...
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU                                   â”‚Copy to GPUâ”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 GPU                                                    â”‚INFER MODEL         â”‚
                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="optimization-read-images-in-parallel-num_workers">Optimization: Read images in parallel <code>num_workers</code></h2>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read image 1â”‚                                 â”‚Read image 4â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE2 â”‚Read image 2â”‚                                 â”‚Read image 5â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE3 â”‚Read image 3â”‚                                 â”‚Read image 6â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU              â”‚Copy to GPUâ”‚                                  â”‚Copy to GPUâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
GPU                                â”‚INFER MODEL         â”‚                         â”‚INFER MODEL         â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="optimization-prefetching-on-cpu-queue-prefetch_factor">Optimization: Prefetching on CPU (Queue) <code>prefetch_factor</code></h2>
<p><strong>This is the defual behavior of Pytroch's dataloader</strong>. It does prefetching on the CPU RAM.</p>
<p>the prefetch_factor parameter of PyTorch DataLoader class. The prefetch_factor parameter only controls CPU-side loading of the parallel data loader processes</p>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read image 1â”‚Read image 4â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE2 â”‚Read image 2â”‚Read image 5â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE3 â”‚Read image 3â”‚Read image 6â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU              â”‚123 to GPUâ”‚                    â”‚456 to GPUâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
GPU                               â”‚INFER 123 into MODELâ”‚          â”‚INFER 456 into MODELâ”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ul>
<li><a href="https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html">But what are PyTorch DataLoaders really?</a></li>
<li><a href="https://teddykoker.com/2020/12/dataloader">Building a Multi-Process Data Loader from Scratch</a></li>
<li>The full code for this project is available at github.com/teddykoker/tinyloader</li>
<li>https://www.jpatrickpark.com/post/loader_sim/</li>
</ul>
<h2 id="optimization-prefetching-on-gpu">Optimization: Prefetching on GPU</h2>
<pre><code>          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE1 â”‚Read image 1â”‚Read image 4â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE2 â”‚Read image 2â”‚Read image 5â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU CORE3 â”‚Read image 3â”‚Read image 6â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
CPU 2 GPU              â”‚123 to GPUâ”‚ â”‚456 to GPUâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
GPU                               â”‚INFER 123 into MODELâ”‚INFER 456 into MODELâ”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ul>
<li>Prefetching Implementation #1: <code>class data_prefetcher()</code> in https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py#L265</li>
<li>
<p>Prefetching Implementation #2: Sacrife 1 data loader process into a prefetcher process</p>
</li>
<li>
<p>https://www.jpatrickpark.com/post/prefetcher/</p>
</li>
<li>https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/</li>
</ul>
<p>Achieving overlap between data transfers and other operations requires the use of CUDA streams, so first letâ€™s learn about streams.</p>
<h2 id="fast-collate-concat-imags-into-batch">Fast collate (= concat imags into batch) ?</h2>
<ul>
<li>https://www.pankesh.com/posts/2019-05-02-pytorch-augmentation-with-libjpeg-turbo/</li>
</ul>
<h2 id="faster-model-tensorrt-engine-into-torchscript-module">Faster Model: TensorRT engine --into--&gt; TorchScript module</h2>
<ul>
<li>https://pytorch.org/TensorRT/</li>
<li>https://pytorch.org/TensorRT/_notebooks/lenet-getting-started.html</li>
<li>https://pytorch.org/TensorRT/py_api/ts.html?highlight=embed#torch_tensorrt.ts.embed_engine_in_new_module</li>
</ul>
<h2 id="summary">Summary</h2>
<p>For getting fast training/inference </p>
<ul>
<li>Data reading:</li>
<li>Use fast data staorage hardware (RAM, NVMe, RAID,...)</li>
<li>Use fast data decoding (libjpeg-turbo for images)</li>
<li>Even faster is you store precomted tensors and load them with either<ul>
<li>Numpy.memmap</li>
<li>torch.Storage</li>
<li>FFIO</li>
</ul>
</li>
<li>Dataloader</li>
<li>Read images in parallel <code>num_workers</code></li>
<li>Avoid unnecessary host copies <code>pin_memory=True</code></li>
<li>Prefetching on CPU (CPU Queue) <code>prefetch_factor</code></li>
<li>Prefetching on GPU (GPU Queue)</li>
<li>Model</li>
<li>TensorRT</li>
</ul>
<h2 id="cuda-programming">CUDA Programming</h2>
<p>Pytorch</p>
<pre><code class="language-python">
my_stream = torch.cuda.Stream()

with torch.cuda.stream(my_stream):

    # Send data to GPU (NO BLOCKING)
    data = data.cuda(non_blocking=True) # or data.to(&quot;cuda&quot;, non_blocking=True)
</code></pre>
<p>PyCUDA</p>
<pre><code class="language-python">
my_stream = cuda.Stream()

# Send data to GPU (NO BLOCKING)
cuda.memcpy_htod_async(dest=gpu_mem[name], src=cpu_mem[name], stream=my_stream)


cuda.memcpy_dtoh_async(dest=cpu_mem[name], src=gpu_mem[name], stream=my_stream)
</code></pre>
<h1 id="copy-betwwnn-numpy-and-pytorch">Copy betwwnn numpy and pytorch</h1>
<table>
<thead>
<tr>
<th></th>
<th>Copy by value, Deep copy</th>
<th>Copy by reference, Shallow copy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numpy to Pytorch</td>
<td><code>torch.tensor(my_npArr)</code></td>
<td><code>torch.from_numpy(my_npArr)</code></td>
</tr>
<tr>
<td>Pytorch to Numpy</td>
<td><code>np.array(my_tensor)</code></td>
<td><code>my_tensor.numpy()</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>my_arr.save(my_file_str)</td>
</tr>
<tr>
<td>my_file = open(filepath, mode='wb'); my_arr.tofile(my_file)</td>
</tr>
<tr>
<td>my_file = open(filepath, mode='wb'); my_file.write(my_arr.tobytes())</td>
</tr>
</tbody>
</table>
<p>https://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../numpy/" class="btn btn-neutral float-left" title="ğŸŸ¦ Numpy"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../numpy/" class="btn btn-neutral float-right" title="ğŸŸ¦ Parallel">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../numpy/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../numpy/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
