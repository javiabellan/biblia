# Web Scraping


## Curl

- `curl -s`: 

## Wget

```bash
wget URL                    # Download and store in the current directory.
wget -O CUSTOM_FILENAME URL # Download and store in the current directory with a different file name.
wget -O - URL               # Download and redirects to stdout
wget --limit-rate=200k URL  # Specify download speed. Here speed is limited to 200k.
wget -c URL                 # Continue the Incomplete Download
wget -b URL                 # Download in the Background
wget --spider URL           # Not download the webpage, just check that it is there.
wget --tries=75 URL         # Increase Total Number of Retry Attempts
wget -i FILE_WITH_URLS.txt  # Download multiple URLs. Each line in the txt document is a URL.
-m --mirror                 # Turns on infinite recursion and time-stamping, and keeps FTP directory listings.
  -r --recursive            # Turn on recursive retrieving. The default maximum depth is 5.
  -l depth --level=depth    # Set the maximum number of subdirectories that Wget will recurse into to depth. inf means infinite
  -N --timestamping         # Turn on time-stamping.
  --no-remove-listing       # Don't remove the .listing files generated by FTP retrievals.
-p --page-requisites      # Downloads all files that are necessary to properly display a given HTML page.
-k --convert-links        # After the download, convert the external links to make the work.
-P ./LOCAL-DIR              # saves all the files and directories to the specified directory.
wget --mirror --page-requisites --convert-links -P ./LOCAL_DIR URL # Download a Full Website
wget -Q5m -i FILE_WITH_URLS.txt   # Quit Downloading When it Exceeds Certain Size
wget --ftp-user=USERNAME --ftp-password=PASSWORD URL # FTP Download With wget
wget --reject=gif URL       # Reject Certain File Types while Downloading
wget -o download.log URL    # Log messages to a log file instead of stderr Using wget -o
```

> ### Reference
> - https://www.queryhome.com/tech/54364/overview-about-wget-command
> - https://www.linuxtechi.com/wget-command-practical-examples/
> - https://www.youtube.com/watch?v=GJum2O2JM6M
> - https://www.youtube.com/watch?v=-GCDJ26B4Ho


## Youtube_dl






## Web Scraping methods

There are 3 approaches of web scrapping:

1. Download static webpages + parse HTML with `pup`
2. Do API reverse engineering + parse JSON with `jq`
3. Use Selenium or Headless browser



### 1. Static webpages

Get a simple static page (server side rendered) with `curl` or `wget` and parse them

Example:

```bash
page_html=$(curl -s $URL)
title=$(echo $page_html | pup 'title' text{})
echo $title

col_A=$(echo $page_html | pup 'a.bb1c_ attr{href}')
col_B=$(echo $page_html | pup 'span.price-sale text{}')
table=$(paste col_A col_B)
```

> https://www.youtube.com/watch?v=GJum2O2JM6M



### 2. API reverse engineering

Useful for dynamic websites. Find the hidden api that makes XHR requests to fullfill the webpage. XMLHttpRequest is used heavily in AJAX programming.

- Step 1: See the requests that the web is doing: Chrome or Firefox dev tools > Network > XHR requests
- Step 2: Copy the target request as cURL (right click)
- Step 3: Paste the target request in [Postman](https://www.postman.com) or [Insomnia](https://insomnia.rest)
- Step 4: Elaborate your own script using `curl` + `jq`


### 3. Headless browser & Selenium

Use Selenium or Headless browser for automate the interaction with the web.

- **Selenium**
- **Google Chrome cli**
- lynx
  - lynx -dump -listonly http://aligajani.com 

  - `google-chrome-stable --headless --disable-gpu --dump-dom 'URL' > ~/file.html`
