# Web Scraping

- `curl`: make any HTTP you want
- `httpie`: like curl but easier (http get)
- `wget`: Download files
- `aria2c`: a fancier wget


## curl

- `-d, --data <data>`: (HTTP) Sends the specified data in a POST request to the HTTP server.
- `-i, --include`: Show HTTP response headers.
- `-s, --silent`: Silent or quiet mode. Don't show progress meter or error messages.
- `-v, --verbose`: Verbose. Show request headers (>) and response headers (<).

## httpie

## wget

```bash
wget URL                    # Download and store in the current directory.
wget -O CUSTOM_FILENAME URL # Download and store in the current directory with a different file name.
wget -O - URL               # Download and redirects to stdout
wget --limit-rate=200k URL  # Specify download speed. Here speed is limited to 200k.
wget -c URL                 # Continue the Incomplete Download
wget -b URL                 # Download in the Background
wget --spider URL           # Not download the webpage, just check that it is there.
wget --tries=75 URL         # Increase Total Number of Retry Attempts
wget -i FILE_WITH_URLS.txt  # Download multiple URLs. Each line in the txt document is a URL.
-m --mirror                 # Turns on infinite recursion and time-stamping, and keeps FTP directory listings.
  -r --recursive            # Turn on recursive retrieving. The default maximum depth is 5.
  -l depth --level=depth    # Set the maximum number of subdirectories that Wget will recurse into to depth. inf means infinite
  -N --timestamping         # Turn on time-stamping.
  --no-remove-listing       # Don't remove the .listing files generated by FTP retrievals.
-p --page-requisites      # Downloads all files that are necessary to properly display a given HTML page.
-k --convert-links        # After the download, convert the external links to make the work.
-P ./LOCAL-DIR              # saves all the files and directories to the specified directory.
wget --mirror --page-requisites --convert-links -P ./LOCAL_DIR URL # Download a Full Website
wget -Q5m -i FILE_WITH_URLS.txt   # Quit Downloading When it Exceeds Certain Size
wget --ftp-user=USERNAME --ftp-password=PASSWORD URL # FTP Download With wget
wget --reject=gif URL       # Reject Certain File Types while Downloading
wget -o download.log URL    # Log messages to a log file instead of stderr Using wget -o
```

> ### Reference
> - https://www.queryhome.com/tech/54364/overview-about-wget-command
> - https://www.linuxtechi.com/wget-command-practical-examples/
> - https://www.youtube.com/watch?v=GJum2O2JM6M
> - https://www.youtube.com/watch?v=-GCDJ26B4Ho



## Login/Session/Cookie









## Hide yoir IP with proxy servers

> - **BAD**: you ---------> request to target
> - **GOOD**: you ---------> requets to proxy server -------> request to target

### 1. [gimmeproxy](https://gimmeproxy.com)

Make the following request:

https://gimmeproxy.com/api/getProxy


### 2. They will provide JSON response with all proxy data which you can use later as needed:
```json
{
  "supportsHttps": true,
  "protocol": "socks5",
  "ip": "179.162.22.82",
  "port": "36915",
  ...
}
```

### 3. Use the proxy with curl
```bash
# -x      [protocol://]host[:port]
# --proxy [protocol://]host[:port]
curl -x socks5://179.162.22.82:36915 http://example.com
```


## Web Scraping methods

There are many approaches of web scrapping:

- Text browser (`lynx`) + parse text (`grep`)
- Download HTML (`cURL`) + to text (`html2text`) + parse text (`grep`)
- Download HTML (`cURL`) + parse HTML (`pup`)
- Do API reverse engineering + parse JSON (`jq`)
- Use Selenium or Headless browser



### Text browser (`lynx`) + parse text (`grep`)

```bash
lynx -dump "$the_url" | grep "$the_str")
```

Reference: https://funprojects.blog/2022/04/07/web-scraping-with-1-line-of-bash/


### 1. Static webpages

Get a simple static page (server side rendered) with `curl` or `wget` and parse them

Example:

```bash
page_html=$(curl -s $URL)
title=$(echo $page_html | pup 'title' text{})
echo $title

col_A=$(echo $page_html | pup 'a.bb1c_ attr{href}')
col_B=$(echo $page_html | pup 'span.price-sale text{}')
table=$(paste col_A col_B)
```

> https://www.youtube.com/watch?v=GJum2O2JM6M



### 2. API reverse engineering

Useful for dynamic websites. Find the hidden api that makes XHR requests to fullfill the webpage. XMLHttpRequest is used heavily in AJAX programming.

- Step 1: See the requests that the web is doing: Chrome or Firefox dev tools > Network > XHR requests
- Step 2: Copy the target request as cURL (right click)
- Step 3: Paste the target request in [Postman](https://www.postman.com) or [Insomnia](https://insomnia.rest)
- Step 4: Elaborate your own script using `curl` + `jq`


### 3. Headless browser & Selenium

Use Selenium or Headless browser for automate the interaction with the web.

- **Selenium**
- **Google Chrome cli**
- lynx
  - lynx -dump -listonly http://aligajani.com 

  - `google-chrome-stable --headless --disable-gpu --dump-dom 'URL' > ~/file.html`





## Refenrences

- https://www.youtube.com/c/JohnWatsonRooney
- https://www.youtube.com/channel/UCBGkweoKCtSBx6GEXRYIWLg/videos
- https://www.youtube.com/playlist?list=PLcUid3OP_4OU0zHx1qlCMdB6pEP1zeaY3
- https://www.youtube.com/watch?v=kmlYp8I1MJs&list=PLISuMnTdVU-xOHf3jEtiK1B_g5HFgXCb-
